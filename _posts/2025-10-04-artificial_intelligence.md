---
title: "인공지능이란?"
layout: single
comments: true
author_profile: true
categories: ['ai']
---


# 인공지능의 분류
일반적으로 인공지능을 분류할 때는 크게 지도 학습(Supervised Learning), 비지도 학습(Unsupervised Learning), 강화 학습(Reinforcement Learning)이 있다고 한다. 추가적으로 Imitation Learning(모방 학습)이라는 것도 존재하긴 하지만 개념이 혼동될 수 있으니 나중에 추가로 post를 만들어서 설명하도록 하겠다👍

일반적으로는 지도 학습과 비지도 학습을 헷갈리는 경우가 존재하고(나만 그런가) 강화 학습은 별도의 개념이라서 쉽게 헷갈리지 않는 것 같다.

## Supervised Learning(지도학습)
우선 Supervised Learning(지도 학습)같은 경우에는 입력(input)과 정답(output)을 제공해 주어 이를 가지고 학습을 진행한다. 

즉 Direct Feedback이 된다는 것인데, 이를 통해서 정답인 label을 명시적으로 제공받는다. 

이런 학습 방식은 새롭게 보지 못한 입력에 대해 정확한 출력 예측이 목표이며, 예를 들어서 사과 사진을 넣어서 사과라고 라벨링을 해주었으면 어떤 사과 사진을 넣어도 사과라고 하는 것이다.

주로 Regression(회귀)와 Classification(분류)로 나뉘는데 회귀는 수치를 넣어서 어떤 사이즈의 생선의 가격은 얼마일 것이다 라고 수치에 대한 값을 예측하는 것이고, 분류는 이게 어떤 사이즈이니 어떤 생선일 것이라고 종류를 맞추는 것이라고 볼 수 있다. 

정답과 입력을 제공하는 지도 학습에는 어떤 것이 있느냐? 라고 한다면 일반적으로 SVM, DNN, CNN, RNN과 같은 것들이 이에 해당한다. 각각이 어떤 기능을 하는지도 역시 추가적으로 post를 작성하도록 할 것 같다.

그래서 저런 친구들을 활용해서 Object Detection, Joint Estimation, Segmentation과 같은 기능들을 할 수 있는데 Object Detection과 Segmentation과 같은 경우에는 라벨 데이터를 제공해줌으로서 어떤 라벨인지 알 수 있게 활용하기 때문에 지도 학습에 해당한다.  
(비지도학습에서 이를 사용하게 된다면 어떤 label에 속하는지는 알기 어렵고 유사한 특징?을 가진 이미지로 Clustering만 가능하다)

지도 학습을 일반적으로 많이 사용하는 가장 큰 이유는 Transfer Learning(전이 학습)이 가능한 것 때문이라고도 볼 수 있는데, 전이 학습 같은 경우에는 사전에 학습된 Weight의 값을 가져오는 것이다. 이러한 부분에 있어서 이미 잘 학습된 가중치를 가져오거나 하여서 자신의 모델에 사용할 수도 있다.  
(물론 이걸 쓴다고 정확도가 올라간다고 장담은 할 수 없지만 원하는 결과에 fitting하는데 도움이 될 수 있다 => 이러면 컴퓨터 리소스를 적게 사용해도 Gradient가 이상한 방향으로 안갈 수도 있음!)  
이러한 Transfer Learning이 가능하기 때문에 일반적으로 직접 학습시키지 않고도 학습된 가중치 yolo 같은 것을 불러올 수 있는 것이다. 

## Unsupervised Learning(비지도 학습)
지도 학습과 다르게 일반적으로 비지도 학습은 label이 없다.  
Q: 그러면 어떻게 학습하나요?  
A: ~~잘~~  
이게 말이 되는가 싶은데 진짜 잘이다. 컴퓨터 맘대로 학습하는거다. 이게 무슨 소리인지 구체적으로 설명하면 어떤 데이터들이 주어졌을때 label이 없어도 데이터 간의 유사도나 분포를 기반으로 스스로 구조를 학습한다. 이 과정에서 모델은 서로 비슷한 데이터의 벡터는 가깝게, 다른 특성을 가진 벡터는 멀어지도록 표현 공간을 형성한다고 볼 수 있따!
물론 사람이 지정한 식에 따라서 가는 것이기 하지만 특정 패턴을 찾아서 컴퓨터가 분류를 해주는 것이라고 볼 수 있다. 

이러한 이유에 있어서 사람이 찾지 못한 데이터 내의 숨겨진 구조와 패턴 분포를 찾을 수 있는데 이를 Hidden Structure라고 한다. 이러한 특징 덕분에 스스로 그룹화하거나 특징을 추출하기 때문에 No Feedback, 피드백을 제공해주지 않아도 된다. 

Clustering과 K-means, PCA, t-SNE, AutoEncoder 등이 이에 해당한다.(자세히 공부를 하고 추가로 정리하도록 하자...)

## Reinforcement Learning(강화 학습)
기존의 지도 학습과 비지도 학습은 다르게 강화 학습은 행동(Action)을 통해 환경(Environment)과 상호작용을 하면서 보상(Reward)을 기반으로 학습한다. 이해가 되지 않는 사람을 위해서 비행기 게임을 예로 들어서 아무것도 모르는 아기가 비행기 게임을 시작했을 때 적에게 맞으면 사탕을 뺏고(?), 미사일로 적 비행기를 격추하면 사탕을 주는 방식이다. 여기서 보스 비행기를 죽이면 대왕 사탕을 주는 등의 방식으로 Reward를 조절할 수 있다.

이를 통해서 최적의 행동 정책(Policy)을 학습해 누적 보상(Maximum Reward)을 극대화해내는 방향으로 학습한다.
여기서 아기, 일반적으로 학습자(Agent)는 시행착오를 통해 배운다. 

이러한 이유에서 강화 학습은 여러 번 도전이 필요하며 이 과정에서 샘플 효율성이 낮고, 보상 설계를 잘 해야 정확한 방향으로 나아갈 수 있다. 로봇 측면에서 보았을 때는 물리적으로 학습을 제대로 할 수 있는가? Feedback없이?가 문제가 될 수 있겠다.  
(이를 해결하기 위해 Imitation Learning을 사용하기도 하는데 이는 나중에 따로 자세히 설명하겠다.)

## 결론
결과적으로 Machine Learning이라는 것에 Supervised Learning과 Unsupervised Learning, Reinforcement Learning이 속하는 것으로 이들은 아래의 표와 같은 차이를 가진다.  
| 구분            | **지도학습 (Supervised Learning)**     | **비지도학습 (Unsupervised Learning)**           | **강화학습 (Reinforcement Learning)**  |
| ------------- | ---------------------------------- | ------------------------------------------- | ---------------------------------- |
| **핵심 개념**     | 정답(Label)이 주어진 데이터를 이용해 학습         | 정답 없이 데이터의 패턴이나 구조를 학습                      | 환경과 상호작용하며 보상(Reward)을 통해 학습       |
| **입력 데이터 형태** | (입력, 정답) 쌍 존재 → (x, y)             | 입력만 존재 → x                                  | 상태(State), 행동(Action), 보상(Reward)  |
| **학습 목표**     | 주어진 입력에 대해 올바른 정답을 예측              | 데이터의 숨겨진 구조나 분포를 파악                         | 보상을 최대화하는 최적의 행동 전략(policy) 학습     |
| **학습 방식**     | 예측 결과와 실제 정답의 오차를 최소화              | 데이터 간 유사성, 거리, 분산 등을 기준으로 구조 학습             | 시행착오(trial & error)를 통해 장기적 보상 극대화 |
| **출력 결과**     | 분류(Classification), 회귀(Regression) | 군집화(Clustering), 차원 축소(Dimension Reduction) | 행동 정책(Policy), 최적의 전략              |
| **정답(Label)** | ✅ 존재                               | ❌ 없음                                        | ⚙️ 보상(Reward)으로 대체                 |
| **피드백 형태**    | 직접적(Direct Feedback) — 정답과 비교 가능   | 없음 (내적 구조를 통해 학습)                           | 간접적(Indirect Feedback) — 보상으로 피드백  |
| **대표 알고리즘**   | 선형회귀, 로지스틱회귀, SVM, CNN, LSTM       | K-means, PCA, Autoencoder, SOM              | Q-learning, SARSA, DQN, PPO        |
| **데이터 필요량**   | 중간 (라벨 필요)                         | 많을수록 좋음                                     | 환경 내 다수의 시도 필요                     |
| **응용 예시**     | 스팸 메일 분류, 이미지 인식, 음성 인식            | 고객군 분류, 차원 축소, 특징 추출                        | 자율주행, 게임 AI, 로봇 제어                 |
| **장점**        | 정확하고 명확한 목표 설정 가능                  | 숨겨진 패턴을 자동으로 발견 가능                          | 실시간으로 의사결정 전략을 최적화                 |
| **단점**        | 라벨링 비용 높음                          | 결과 해석 어려움                                   | 학습 불안정, 시도 시간 길음                   |

### 틀린 내용이나 궁금한 내용이 있으면 아래 댓글로 남겨주세요😎